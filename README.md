<h1> Image Animation (&BG SWAP) with Perturbed Masks </h1>


This repo presents an extensions to the method in paper Image Animation with Perturbed Masks - Yoav Shalev, Lior Wolf.
Paper -

The extended model receives an optional third image from the same type (but any identity) and the background of this third image is merged in the animated source image by the driving video.


<h2> Approach </h2>

<b> Keypoint </b> - Use the masks generated by the method in order to segment the foreground from the background of two frames and merge the results .


A 2-step generator which consists of 2 Encoder Decoder based models was trained in a self-supervised manner in order to merge the results. As in the paper, the networks were trained using two frames from the same video.


Denote by   m(x) the output of the mask generator for image x. Notice x is 256x256 but m(x) is 64x64.
            bi(x) the binary mask obtained by thresholding m(x).
            U(x) and D(x) the upsampled and downsampled version of x, obtained via bilinear interpolation.
            F(x, bi(x)) and B(x, bi(x)) the foreground and background segmented respectively w.r.t to the binary mask bi(x).


For a source image X and a driving image Y of size 256x256 -


The first model BGBase receives 8 channels of 64x64 images as input and outputs one low-resolution image of size 256x256.


out = BGBase(  concat( m(X), m(Y), F(D(Y), bi(Y)), F(D(X), bi(X)) ))


The second model BGRefine receives 11 channels of 256x256 images as input and outputs one high-resolution image of size 256x256.


out_hd = BGRefine(out, U(m(X)), U(m(Y)),  F(Y, U(bi(Y))), F(X, U(bi(X)))


Each of the network is trained to minimize the multi-scale vgg perceptual loss (identically to paper)
between its output and the driving target image (the networks "see" only the foreground of this image).
The masks inputs m(X) and m(Y) are detached from their computational graph and passed to the models s.t this extension does not affect the original method and its results.
The networks BGBase and BGRefine are also trained independently from each other , and backpropagation occurs in each network separately.

isolated image
original image

<h2> Inference
At inference time, the background extension model depends on images from the same distribution and their masks generated by network m in paper, these were extensively seen during training.
The model also depends on network r output (mask refinement from perturbations in paper), but as this network has been trained to minimize the L1 distance between its output and mask generator m's output, it's fine.
The most challenging part at inference time will be to swap backgrounds between frames with different original backgrounds, which we
